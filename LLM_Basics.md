 - There are LLMs, ChatModels, Text embedding models.
 -
Async IO important shit

- Have to use concurrent funtions to effficiently get the data required.
- We have a fake LLM that can be played around with to figure out to how to deal with different type of responses
- We have different type of cache solutions whether it be using SQL to store the data, or puting the data in memory.
- 